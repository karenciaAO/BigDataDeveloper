{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef130b84-2f40-4d4a-b193-6289cb4a4523",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pyspark\n",
    "\n",
    "### Agenda \n",
    " \n",
    "1. Introduccion\n",
    "2. Arquitectura\n",
    "3. Tipos de Clusters\n",
    "4. Modulos de Spark\n",
    "5. Spark RDD\n",
    "6. Spark DataFrame\n",
    "7. Spark SQL\n",
    "8. Spark Streaming\n",
    "9. Spark GrpahFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: pip in /usr/local/python/3.12.1/lib/python3.12/site-packages (25.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5e634f9-8165-4596-bd31-f5e05eebde19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduccion\n",
    "\n",
    "PySpark es una biblioteca de Spark escrita en Python para ejecutar aplicaciones de Python usando las capacidades de Apache Spark, con PySpark podemos ejecutar aplicaciones en paralelo en el clúster distribuido (múltiples nodos).\n",
    "En otras palabras, PySpark es una API de Python para Apache Spark. Apache Spark es un motor de procesamiento analítico para potentes aplicaciones de aprendizaje automático y procesamiento de datos distribuidos a gran escala.\n",
    "\n",
    "##### ¿Quién utiliza Pyspark?\n",
    "PySpark se usa muy bien en la comunidad de ciencia de datos y aprendizaje automático, ya que hay muchas bibliotecas de ciencia de datos ampliamente utilizadas escritas en Python, incluidas NumPy, TensorFlow. También se utiliza debido a su procesamiento eficiente de grandes conjuntos de datos. PySpark ha sido utilizado por muchas organizaciones como Walmart, Trivago, Sanofi, Runtastic y muchas más.\n",
    "\n",
    "##### caracterísicas importantes de Spark\n",
    "![](https://sparkbyexamples.com/wp-content/uploads/2020/08/pyspark-features-1.png)\n",
    "# \n",
    "- Cálculo en memoria\n",
    "- Procesamiento distribuido usando paralelizar\n",
    "- Se puede usar con muchos administradores de clústeres (Spark, Yarn, Mesos, etc.)\n",
    "- Tolerante a fallos\n",
    "- Inmutable\n",
    "- Evaluación perezosa\n",
    "- Caché y persistencia\n",
    "- Optimización incorporada al usar DataFrames\n",
    "- Soporta ANSI SQL\n",
    "\n",
    "##### Ventajas\n",
    "#\n",
    "- Spark es un motor de procesamiento distribuido, en memoria y de uso general que le permite procesar datos de manera eficiente y distribuida.\n",
    "- Obtendrá grandes beneficios al usar Spark para canalizaciones de ingesta de datos.\n",
    "- Con Spark podemos procesar datos de Hadoop HDFS, AWS S3 y muchos sistemas de archivos.\n",
    "- PySpark también se usa para procesar datos en tiempo real usando Streaming y Kafka.\n",
    "- Con spark streaming, también puede transmitir archivos desde el sistema de archivos y también desde el socket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc13b97c-6e22-49ca-b945-efd7522cb938",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Arquitectura\n",
    "# \n",
    "Apache Spark funciona en una arquitectura master-slave donde el master se llama \"driver\" y los esclavos se llaman \"Workers\". Cuando ejecuta una aplicación Spark, Spark Driver crea un contexto que es un punto de entrada a su aplicación, y todas las operaciones (transformaciones y acciones) se ejecutan en los workers y los recursos son administrados por el Administrador de clústeres.\n",
    "\n",
    "![](https://sparkbyexamples.com/wp-content/uploads/2020/02/spark-cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c710ab46-0bd0-4e2e-94a8-d573e24ace0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Tipos de clusters\n",
    "#\n",
    "- Standalone –  un administrador de clúster simple incluido con Spark que facilita la configuración de un clúster.\n",
    "- Apache Mesos – Mesos es un administrador de clústeres que también puede ejecutar aplicaciones Hadoop MapReduce y PySpark.\n",
    "- Hadoop YARN –  el administrador de recursos en Hadoop 2. Esto se usa principalmente como administrador de clústeres.\n",
    "- Kubernetes – un sistema de código abierto para automatizar la implementación, el escalado y la gestión de aplicaciones en contenedores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e08b3519-519b-48ff-ac3d-49a903e3d192",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Módulos de Spark\n",
    "\n",
    "![](https://tse1.mm.bing.net/th?id=OIP.IH66oypTYnWQjFkFHoBMCgHaD2&pid=Api&P=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09fbe660-9e52-46cd-b766-f32fe0269c1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark RDD\n",
    "\n",
    "RDD es una estructura de datos fundamental de Spark que es una colección de objetos distribuidos inmutables y tolerantes a fallas, lo que significa que una vez que crea un RDD no puede cambiarlo. Cada conjunto de datos en RDD se divide en particiones lógicas, que se pueden calcular en diferentes nodos del clúster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tolerante a fallos por que rdd que se crea no se modifica y se distribule en los nodos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ae8225-89ee-400e-a966-17fbea3f26e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark\n",
    "#para trabajar con spark se crea una spark sesion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2d401e-1301-46b2-8266-032be599e883",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"RDD\") \\\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "un rdd se crea con un paralelize tambine con una lista de tupla o con un text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de2afa44-0561-4034-8330-b0bc9d7ac31b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Java', 20000), ('Python', 100000), ('Scala', 3000)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creacion de un RDD utilizando parallelize\n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd = spark.sparkContext.parallelize(dataList)\n",
    "rdd.collect() #accion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b7fcec-8c9f-4626-94ff-ec2271aa8adc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = spark.sparkContext.textFile(\"/workspaces/BigDataDeveloper/python/test_.txt\")\n",
    "rdd2.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e149a9ad-01dc-4439-9d37-6fa33eb064e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Operaciones RDD\n",
    "En PySpark RDD, puede realizar dos tipos de operaciones.\n",
    "\n",
    "Transformaciones RDD:  las transformaciones son operaciones perezosas. Cuando ejecuta una transformación (por ejemplo, una actualización), en lugar de actualizar un RDD actual, estas operaciones devuelven otro RDD.\n",
    "\n",
    "Acciones de RDD  : operaciones que activan el cálculo y devuelven valores de RDD al controlador.\n",
    "\n",
    "##### Transformaciones RDD\n",
    "Las transformaciones en Spark RDD  devuelven otro RDD y las transformaciones son perezosas, lo que significa que no se ejecutan hasta que llamas a una acción en RDD. Algunas transformaciones en los RDD son flatMap(), map(), reduceByKey(), filter(), sortByKey() y devuelven un nuevo RDD en lugar de actualizar el actual.\n",
    "\n",
    "##### Acciones de RDD\n",
    "La operación Acción de RDD devuelve los valores de un RDD a un nodo de controlador. En otras palabras, cualquier función RDD que no devuelva RDD[T] se considera una acción. \n",
    "\n",
    "Algunas acciones en RDD son count(), collect(), first(), max()y reduce() y más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#que sonnoperaciones peresosas?\n",
    "#las tranformacion pueden ser par mapeo lectura filtro etc\n",
    "#cada tranformacion es un grafo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d20386e9-2c95-4d49-8bc1-8f9d3e6453bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark DataFrame\n",
    "\n",
    "DataFrame es una colección distribuida de datos organizados en columnas con nombre. Es conceptualmente equivalente a una tabla en una base de datos relacional o un marco de datos en R/Python, pero con optimizaciones más ricas bajo el capó. Los marcos de datos se pueden construir a partir de una amplia gama de fuentes, como archivos de datos estructurados, tablas en Hive, bases de datos externas o RDD existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8403872-1c81-46f8-a42c-d9c266d0a785",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creación de un dataframe utilizando createDataFrame\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a6945d0-6198-4ffc-bc14-a9bd7f1bb7a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display(df) \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7777187f-9a83-4e9c-bbcd-5d37015180a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creación de un dataframe utilizando una fuente externa\n",
    "df = spark.read.csv(\"/workspaces/BigDataDeveloper/python/source/tmp/resources/zipcodes.csv\") # spark.read.json/ .parquet ( spark.read.format().load())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6639e965-86d4-4f59-8115-5588db9c5c37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark SQL\n",
    "\n",
    "Spark SQL en Apache Spark es uno de los módulos para el procesamiento de la información que ofrece Apache Spark y que trabaja con datos estructurados.\n",
    "Una vez que haya creado un DataFrame, puede interactuar con los datos utilizando la sintaxis SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0619ae59-15c7-44ec-b3e7-71c2d89cbfb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 03:28:12 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|         _c0|    _c1|        _c2|                _c3|  _c4|           _c5|  _c6|    _c7|  _c8|  _c9| _c10|       _c11|   _c12|                _c13|                _c14|         _c15|           _c16|               _c17|      _c18|         _c19|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|   Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|        Notes|\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        FALSE|           NULL|               NULL|      NULL|         NULL|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        FALSE|           NULL|               NULL|      NULL|         NULL|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           NULL|               NULL|      NULL|         NULL|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72| -97.31| -0.1|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        FALSE|           NULL|               NULL|      NULL|         NULL|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        FALSE|           2126|               4053| 122396986|         NULL|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|        Ft Worth, TX|   NA-US-TX-FT WORTH|        FALSE|           2126|               4053| 122396986|         NULL|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US| Urb Eugene Rice, PR|NA-US-PR-URB EUGE...|        FALSE|           NULL|               NULL|      NULL|         NULL|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|33.37|-111.64| -0.3|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14962|              26883| 563792730|no NWS data, |\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|33.38|-111.84|-0.31|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14374|              25446| 471000465|         NULL|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|30.69| -81.92| 0.12|-0.85| 0.51|         NA|     US|        Hilliard, FL|   NA-US-FL-HILLIARD|        FALSE|           3922|               7443| 133112149|         NULL|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|28.96| -82.41| 0.11|-0.86| 0.48|         NA|     US|          Holder, FL|     NA-US-FL-HOLDER|        FALSE|           NULL|               NULL|      NULL|         NULL|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|30.72| -86.67| 0.04|-0.85| 0.51|         NA|     US|            Holt, FL|       NA-US-FL-HOLT|        FALSE|           1207|               2190|  36395913|         NULL|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|28.78| -82.61| 0.11|-0.86| 0.48|         NA|     US|       Homosassa, FL|  NA-US-FL-HOMOSASSA|        FALSE|           NULL|               NULL|      NULL|         NULL|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           NULL|               NULL|      NULL|         NULL|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|   Sect Lanausse, PR|NA-US-PR-SECT LAN...|        FALSE|           NULL|               NULL|      NULL|         NULL|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|33.97| -85.55| 0.06|-0.82| 0.55|         NA|     US|   Spring Garden, AL|NA-US-AL-SPRING G...|        FALSE|           NULL|               NULL|      NULL|         NULL|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|33.77| -86.47| 0.05|-0.82| 0.55|         NA|     US|     Springville, AL|NA-US-AL-SPRINGVILLE|        FALSE|           4046|               7845| 172127599|         NULL|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|34.37| -87.69| 0.03|-0.82| 0.56|         NA|     US|     Spruce Pine, AL|NA-US-AL-SPRUCE PINE|        FALSE|            610|               1209|  18525517|         NULL|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE| 36.4| -80.56| 0.13|-0.79| 0.59|         NA|     US|        Ash Hill, NC|   NA-US-NC-ASH HILL|        FALSE|            842|               1666|  28876493|         NULL|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"PERSON_DATA\")\n",
    "df.cache()\n",
    "df.count()\n",
    "df2 = spark.sql(\"SELECT * from PERSON_DATA\")\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0145ee84-c4b0-4da5-a1f4-0a9304cd5718",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark Streaming\n",
    "Spark Streaming es un sistema de procesamiento de transmisión escalable, de alto rendimiento y tolerante a fallas que admite cargas de trabajo tanto por lotes como de transmisión. Se utiliza para procesar datos en tiempo real de fuentes como la carpeta del sistema de archivos, el socket TCP, S3 , Kafka , Flume , Twitter y Amazon Kinesis , por nombrar algunos. Los datos procesados se pueden enviar a bases de datos, Kafka, dashboard, etc.\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/streaming-arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12f7e519-1b2a-4254-af0e-fedc5c66962a",
     "showTitle": true,
     "title": "Streaming con socket"
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.readStream\n",
    "      .format(\"socket\")\n",
    "      .option(\"host\",\"localhost\")\n",
    "      .option(\"port\",\"9091\")\n",
    "      .load()\n",
    ")        \n",
    "query = (df.writeStream\n",
    "      .format(\"console\")\n",
    "      .outputMode(\"update\")\n",
    "      .start()\n",
    ")\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7725bf95-6aba-4ed7-be82-42d4aacd1ed2",
     "showTitle": true,
     "title": "Streaming con kafka"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1616466209.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    .format(\"kafka\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "df = spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\")\n",
    "        .option(\"subscribe\", \"json_topic\")\n",
    "        .option(\"startingOffsets\", \"earliest\") // From starting\n",
    "        .load()\n",
    "        \n",
    "df.selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "   .writeStream\n",
    "   .format(\"kafka\")\n",
    "   .outputMode(\"append\")\n",
    "   .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\")\n",
    "   .option(\"topic\", \"josn_data_topic\")\n",
    "   .start()\n",
    "   .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79195a12-e27e-4dbf-b8db-0ab2a2787cdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark GrpahFrame\n",
    "\n",
    "GraphFrames es un paquete para Apache Spark que proporciona gráficos basados ​​en DataFrame. Proporciona API de alto nivel en Scala, Java y Python. Su objetivo es proporcionar tanto la funcionalidad de GraphX como la funcionalidad extendida aprovechando Spark DataFrames. Esta funcionalidad ampliada incluye búsqueda de motivos, serialización basada en DataFrame y consultas gráficas altamente expresivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Introduccion",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
